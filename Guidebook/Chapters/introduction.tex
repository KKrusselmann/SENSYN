%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\label{introduction}

Open Science is a movement to promote and stimulate open and accessible research practices, not just at the stage of publishing research results, but throughout the research cycle as a whole: from preregistration of research ideas and processes, to open analysis protocols, open code, open data, open peer review, open access publications and all steps in between. These practices have the aim to make research more accessible and transparent, to foster collaboration, strengthen trust in research and increase the scientific and societal impact of research. This movement is gaining momentum globally, driven by the growing recognition of the value of open science to research for advancing knowledge and addressing societal challenges \cite{UNESCO}.

An integral part of every empirical research cycle is data, which, too, is part of open research practices. Open data and FAIR data are central to the ethos of open science. \textbf{Open data} refers to data that is freely available for anyone to use, reuse and redistribute, with as little restrictions as possible. \textbf{FAIR data}, on the other hand, emphasises that data should be \textbf{f}indable, \textbf{a}ccessible, \textbf{i}nteroperable, and \textbf{r}esuable. The concept of FAIR data was introduced to create an infrastructure around the reuse of scholarly data. While open data focuses on the unrestricted availability of data, FAIR data principles provide a framework for ensuring that data are not only accessible, but also well-organised, annotated, and structured in ways that make them useful and meaningful to researchers across disciplines. Thus, open and FAIR data are not synonyms: data can be open but not follow FAIR standards, whilst FAIR standards do not require data to be open without restrictions. Specifically, FAIR standards recognise that not all data can be made open and accessible to everyone at all times. Therefore, when it comes to open data, the principle "as open as possible, as restricted as necessary" is commonly applied, which contrasts the ideas of open data \cite{wilkinson2016fair}. 

Both open data practices and FAIR principles are associated with several benefits to researchers. By making data openly available, researchers can enhance the visibility and impact of their work, leading to increased citations and greater recognition within the scientific community. FAIR data practices improve data quality and facilitate data sharing and collaboration, enabling researchers to build on each other's work more effectively. This collaborative approach can lead to new insights, help to avoid duplication of data collection efforts, and accelerate the generation of new knowledge. Furthermore, open and FAIR data practices support transparency and reproducibility in research, which in return should enhance trust in the scientific process and results. In addition, more and more journals and funding agencies require researchers to share (meta)data or a justification for why data sharing is not possible. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hurdles for datasharing}

Despite these benefits, some researchers are reluctant to adopt open and FAIR data practices. There may be concerns about the misuse of data, loss of competitive advantage, and the potential for data to be misinterpreted or misrepresented when made openly accessible. Another concern about open and FAIR data sharing may relate to legal and ethical risks: The processing and sharing of personal data is protected under privacy regulations, such as the European General Data Protection Regulations (GDPR). The GDPR defines personal data as "any information relating to an identified or identifiable natural person ('data subject')" \cite{gdpr2016general}, which includes a wide range of information, from names and addresses to someone's gender or age, biometric data, and social identity markers. Some researchers may be faced with even more restrictive regulations, for example when working with highly sensitive data, such as crime-, financial- or health- data. In the face of these restrictions, researchers may opt out of sharing any part of their data.  

Yet, the consequences of a lack of data sharing cannot be understated, in particular in those disciplines that commonly work with sensitive and/or personal data. Pridemore, for example, notes a replication crisis in the field of criminology, which is partially brought about by the unwillingness of researchers to share their data and a lack of a culture that incentivises such practices, with potentially far-reaching consequences for crime and justice policies \cite{pridemore2018replication}. When it comes to societal impact of research, a lack of transparency may decrease trust in research and evidence-based policies \cite{freese2022advances,powers2019open}. 

In light of these and other issues resulting from a lack of sharing (sensitive) data, the question arises whether there are options to share data of personal and sensitive nature, whilst still protecting the privacy of research subjects and adhering to relevant regulations. With fast developments in artificial intelligence and machine learning  over the last few years, promising tools arose that offer new approaches of processing, and eventually sharing, data without violating privacy regulations. One of these tools is the concept of synthetic data, which is, in short, artificially manufactured data that mimics real data without referring to actual persons that are protected through GDPR and other regulations \cite{el2020practical,hradec2022multipurpose,jordon2022synthetic} (more on synthetic data in the next chapter). Although the idea of synthetic data has gained traction amongst statisticians, computer- and data-scientists and has been applied by some commercial and public organisations, it has not yet reached all academic disciplines (specifically in the social sciences and humanities). One main reason for the lack of attention on synthetic data in those fields is that the majority of discourse on synthetic data has focused on the underlying statistical and technical processes of data synthesis, rather than the application of synthetic data to various fields. In addition, existing literature on synthetic data is somewhat inaccessible as it requires advanced technical and statistical knowledge. As a result, researchers without the technical expertise to engage with existing literature may not be aware of options to making their personal data FAIR and open.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{About this guidebook}
\label{intro:about}

This guidbooke aims to introduce and stimulate the use of synthetic data as a promising tool to overcome obstacles to the sharing of sensitive data for public and research use, in particular to a non-technical audience. 
This guidebook includes a general introduction to the concept, possible applications, a non-technical explanation of the synthesis process, an introduction to open-source tools for data synthesis and an overview of how the privacy and utility of synthetic data needs to be evaluated.

\href{https://www.universiteitleiden.nl/en/research/research-projects/governance-and-global-affairs/project-sensyn#tab-1}{Researchers from Leiden University and the Leiden University Medical Centre} created this report in the context of Project SENSYN, which is funded through the \href{https://www.nwo.nl/onderzoeksprogrammas/open-science/open-science-fund}{NWO Open Science Fund}. More information on the project and its outputs can be found on the \href{https://www.universiteitleiden.nl/en/research/research-projects/governance-and-global-affairs/project-sensyn#tab-1}{project page of Leiden University} or the \href{https://dutchhomicide.streamlit.io}{project's web application}.

